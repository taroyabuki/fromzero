{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09\n",
    "\n",
    "## 9.1 アヤメのデータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "my_data = sm.datasets.get_rdataset('iris', 'datasets').data\n",
    "my_data.head()\n",
    "#>    Sepal.Length  Sepal.Width  Petal.Length  Petal.Width Species\n",
    "#> 0           5.1          3.5           1.4          0.2  setosa\n",
    "#> 1           4.9          3.0           1.4          0.2  setosa\n",
    "#> 2           4.7          3.2           1.3          0.2  setosa\n",
    "#> 3           4.6          3.1           1.5          0.2  setosa\n",
    "#> 4           5.0          3.6           1.4          0.2  setosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.describe()\n",
    "#>        Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n",
    "#> count    150.000000   150.000000    150.000000   150.000000\n",
    "#> mean       5.843333     3.057333      3.758000     1.199333\n",
    "# 以下省略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 木による分類\n",
    "\n",
    "### 9.2.1 分類木の作成と利用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn import tree\n",
    "\n",
    "my_data = sm.datasets.get_rdataset('iris', 'datasets').data\n",
    "X, y = my_data.iloc[:, 0:4], my_data.Species\n",
    "\n",
    "my_model = tree.DecisionTreeClassifier(max_depth=2, random_state=0)\n",
    "my_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dot = tree.export_graphviz(\n",
    "    decision_tree=my_model,\n",
    "    out_file=None,                 # ファイルに出力しない．\n",
    "    feature_names=X.columns,       # 変数名\n",
    "    class_names=my_model.classes_, # カテゴリ名\n",
    "    filled=True)                   # 色を塗る．\n",
    "graphviz.Source(my_dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_test = pd.DataFrame([[5.0, 3.5, 1.5, 0.5],\n",
    "                        [6.5, 3.0, 5.0, 2.0]])\n",
    "my_model.predict(my_test)\n",
    "#> array(['setosa', 'virginica'], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    my_model.predict_proba(my_test),\n",
    "    columns=my_model.classes_)\n",
    "#>    setosa  versicolor  virginica\n",
    "#> 0     1.0    0.000000   0.000000\n",
    "#> 1     0.0    0.021739   0.978261"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 正解率\n",
    "\n",
    "### 9.3.1 混同行列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, LeaveOneOut\n",
    "\n",
    "my_data = sm.datasets.get_rdataset('iris', 'datasets').data\n",
    "X, y = my_data.iloc[:, 0:4], my_data.Species\n",
    "\n",
    "my_model = tree.DecisionTreeClassifier(max_depth=2, random_state=0).fit(X, y)\n",
    "y_ = my_model.predict(X)\n",
    "confusion_matrix(y_true=y, y_pred=y_)\n",
    "#> array([[50,  0,  0],\n",
    "#>        [ 0, 49,  1],\n",
    "#>        [ 0,  5, 45]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.2 正解率（訓練）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.score(X, y)\n",
    "# あるいは\n",
    "y_ = my_model.predict(X)\n",
    "(y_ == y).mean()\n",
    "\n",
    "#> 0.96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.3 正解率（検証）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(my_model, X, y, cv=LeaveOneOut()).mean()\n",
    "#> 0.9533333333333334"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.4 パラメータチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_search = GridSearchCV(estimator=tree.DecisionTreeClassifier(random_state=0),\n",
    "                         param_grid={'max_depth': range(1, 11)},\n",
    "                         cv=LeaveOneOut(),\n",
    "                         n_jobs=-1).fit(X, y)\n",
    "my_search.best_params_, my_search.best_score_\n",
    "#> ({'max_depth': 2}, 0.9533333333333334)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.5 補足：木の複雑さの制限"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_params = {\n",
    "    'max_depth': range(2, 6),\n",
    "    'min_samples_split': [2, 20],\n",
    "    'min_samples_leaf': range(1, 8)}\n",
    "\n",
    "my_search = GridSearchCV(\n",
    "    estimator=tree.DecisionTreeClassifier(min_impurity_decrease=0.01,\n",
    "                                          random_state=0),\n",
    "    param_grid=my_params,\n",
    "    cv=LeaveOneOut(),\n",
    "    n_jobs=-1).fit(X, y)\n",
    "my_search.best_params_, my_search.best_score_\n",
    "#> ({'max_depth': 3, 'min_samples_leaf': 5, 'min_samples_split': 2},\n",
    "#>  0.9733333333333334)\n",
    "\n",
    "tmp = my_search.cv_results_\n",
    "my_results = pd.DataFrame(tmp['params']).assign(\n",
    "    Accuracy=tmp['mean_test_score'])\n",
    "# 正解率（検証）の最大値\n",
    "my_results[my_results.Accuracy == my_results.Accuracy.max()]\n",
    "#>     max_depth  min_samples_leaf  min_samples_split  Accuracy\n",
    "#> 22          3                 5                  2  0.973333\n",
    "#> 23          3                 5                 20  0.973333\n",
    "#> 36          4                 5                  2  0.973333\n",
    "#> 37          4                 5                 20  0.973333\n",
    "#> 50          5                 5                  2  0.973333\n",
    "#> 51          5                 5                 20  0.973333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = my_search.best_estimator_\n",
    "my_dot = tree.export_graphviz(\n",
    "    decision_tree=my_model,\n",
    "    out_file=None,\n",
    "    feature_names=X.columns,\n",
    "    class_names=my_model.classes_,\n",
    "    filled=True)\n",
    "graphviz.Source(my_dot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 複数の木を使う方法\n",
    "\n",
    "### 9.4.1 ランダムフォレスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "import xgboost\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, LeaveOneOut\n",
    "\n",
    "my_data = sm.datasets.get_rdataset('iris', 'datasets').data\n",
    "X, y = my_data.iloc[:, 0:4], my_data.Species\n",
    "\n",
    "my_search = GridSearchCV(RandomForestClassifier(),\n",
    "                         param_grid={'max_features': [2, 3, 4]},\n",
    "                         cv=LeaveOneOut(),\n",
    "                         n_jobs=-1).fit(X, y)\n",
    "my_search.best_params_\n",
    "#> {'max_features': 2}\n",
    "\n",
    "my_search.cv_results_['mean_test_score']\n",
    "#> array([0.96      , 0.96      , 0.95333333])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4.2 ブースティング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore', UserWarning) # これ以降，警告を表示しない．\n",
    "my_search = GridSearchCV(\n",
    "    xgboost.XGBClassifier(eval_metric='mlogloss'),\n",
    "    param_grid={'n_estimators'    : [50, 100, 150],\n",
    "                'max_depth'       : [1, 2, 3],\n",
    "                'learning_rate'   : [0.3, 0.4],\n",
    "                'gamma'           : [0],\n",
    "                'colsample_bytree': [0.6, 0.8],\n",
    "                'min_child_weight': [1],\n",
    "                'subsample'       : [0.5, 0.75, 1]},\n",
    "    cv=5, # 5分割交差検証\n",
    "    n_jobs=1).fit(X, y) # n_jobs=-1ではない．\n",
    "warnings.simplefilter('default', UserWarning) # これ以降，警告を表示する．\n",
    "\n",
    "my_search.best_params_\n",
    "#> {'colsample_bytree': 0.6,\n",
    "#>  'gamma': 0,\n",
    "#>  'learning_rate': 0.3,\n",
    "#>  'max_depth': 1,\n",
    "#>  'min_child_weight': 1,\n",
    "#>  'n_estimators': 50,\n",
    "#>  'subsample': 0.75}\n",
    "\n",
    "my_search.best_score_\n",
    "#> 0.9666666666666668"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4.3 入力変数の重要度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = RandomForestClassifier().fit(X, y)\n",
    "tmp = pd.Series(my_model.feature_importances_, index=X.columns)\n",
    "tmp.sort_values().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.5 欠損のあるデータでの学習\n",
    "\n",
    "### 9.5.1 欠損のあるデータの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "import xgboost\n",
    "from sklearn import tree\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score, LeaveOneOut\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "my_data = sm.datasets.get_rdataset('iris', 'datasets').data\n",
    "\n",
    "n = len(my_data)\n",
    "my_data['Petal.Length'] = [np.nan if i % 10 == 0 else\n",
    "                           my_data['Petal.Length'][i] for i in range(n)]\n",
    "my_data['Petal.Width']  = [np.nan if i % 10 == 1 else\n",
    "                           my_data['Petal.Width'][i]  for i in range(n)]\n",
    "\n",
    "my_data.describe() # countの値が135の変数に，150-135=15個の欠損がある．\n",
    "#>        Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\n",
    "#> count    150.000000   150.000000    135.000000   135.000000\n",
    "#> mean       5.843333     3.057333      3.751852     1.197037\n",
    "# 以下省略\n",
    "\n",
    "X, y = my_data.iloc[:, 0:4], my_data.Species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5.2 方針1：欠損を埋めて学習する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')), # 欠損を中央値で埋める．\n",
    "    ('tree', tree.DecisionTreeClassifier(random_state=0))])\n",
    "my_scores = cross_val_score(my_pipeline, X, y, cv=LeaveOneOut(), n_jobs=-1)\n",
    "my_scores.mean()\n",
    "#> 0.9333333333333333"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5.3 方針2：欠損があっても使える手法で学習する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore', UserWarning)  # これ以降，警告を表示しない．\n",
    "my_scores = cross_val_score(\n",
    "    xgboost.XGBClassifier(eval_metric='mlogloss'), X, y, cv=5)\n",
    "warnings.simplefilter('default', UserWarning) # これ以降，警告を表示する．\n",
    "\n",
    "my_scores.mean()\n",
    "#> 0.9666666666666668"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.6 他の分類手法\n",
    "\n",
    "### 9.6.1 K最近傍法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import cross_val_score, LeaveOneOut\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "my_data = sm.datasets.get_rdataset('iris', 'datasets').data\n",
    "X, y = my_data.iloc[:, 0:4], my_data.Species\n",
    "\n",
    "my_scores = cross_val_score(KNeighborsClassifier(), X, y, cv=LeaveOneOut())\n",
    "my_scores.mean()\n",
    "#> 0.9666666666666667"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.6.2 ニューラルネットワーク"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import cross_val_score, LeaveOneOut\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "my_data = sm.datasets.get_rdataset('iris', 'datasets').data\n",
    "X, y = my_data.iloc[:, 0:4], my_data.Species\n",
    "\n",
    "my_pipeline = Pipeline([('sc', StandardScaler()),  # 標準化\n",
    "                        ('mlp', MLPClassifier())]) # ニューラルネットワーク\n",
    "my_scores = cross_val_score(my_pipeline, X, y, cv=LeaveOneOut(), n_jobs=-1)\n",
    "my_scores.mean()\n",
    "#> 0.9533333333333334"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 }
}
